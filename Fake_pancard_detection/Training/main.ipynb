{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4cfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af1504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71fa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = datasets.ImageFolder(root='train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00abf86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use a pretrained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Binary classification\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35628fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9617829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2030\n",
      "Epoch 2/10, Loss: 0.1944\n",
      "Epoch 3/10, Loss: 0.1738\n",
      "Epoch 4/10, Loss: 0.1773\n",
      "Epoch 5/10, Loss: 0.1375\n",
      "Epoch 6/10, Loss: 0.1274\n",
      "Epoch 7/10, Loss: 0.1120\n",
      "Epoch 8/10, Loss: 0.1360\n",
      "Epoch 9/10, Loss: 0.1119\n",
      "Epoch 10/10, Loss: 0.1183\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e99d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as cnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'cnn_model.pth')\n",
    "print(\"Model saved as cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b9a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model (including architecture)\n",
    "torch.save(model, 'cnn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18b403c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6bb47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.0811\n",
      "Validation - Acc: 0.9220, Precision: 0.7541, Recall: 0.9787, F1: 0.8519\n",
      "[Epoch 2] Train Loss: 0.0681\n",
      "Validation - Acc: 0.9659, Precision: 0.9167, Recall: 0.9362, F1: 0.9263\n",
      "[Epoch 3] Train Loss: 0.0561\n",
      "Validation - Acc: 0.9707, Precision: 0.9184, Recall: 0.9574, F1: 0.9375\n",
      "[Epoch 4] Train Loss: 0.0561\n",
      "Validation - Acc: 0.9659, Precision: 0.9348, Recall: 0.9149, F1: 0.9247\n",
      "[Epoch 5] Train Loss: 0.0518\n",
      "Validation - Acc: 0.9659, Precision: 0.9000, Recall: 0.9574, F1: 0.9278\n",
      "[Epoch 6] Train Loss: 0.0595\n",
      "Validation - Acc: 0.9415, Precision: 0.8182, Recall: 0.9574, F1: 0.8824\n",
      "[Epoch 7] Train Loss: 0.0769\n",
      "Validation - Acc: 0.9415, Precision: 0.8182, Recall: 0.9574, F1: 0.8824\n",
      "[Epoch 8] Train Loss: 0.0939\n",
      "Validation - Acc: 0.9024, Precision: 0.7213, Recall: 0.9362, F1: 0.8148\n",
      "[Epoch 9] Train Loss: 0.1103\n",
      "Validation - Acc: 0.9122, Precision: 0.7736, Recall: 0.8723, F1: 0.8200\n",
      "[Epoch 10] Train Loss: 0.0869\n",
      "Validation - Acc: 0.9366, Precision: 0.8148, Recall: 0.9362, F1: 0.8713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # Convert logits to class\n",
    "            all_preds.extend(preds.astype(int).flatten())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation - Acc: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "943e8ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Final Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7114e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (Text Format):\n",
      "True Positives (Real as Real)   | 44\n",
      "False Positives (Fake as Real)  | 10\n",
      "False Negatives (Real as Fake)  | 3\n",
      "True Negatives (Fake as Fake)   | 148\n",
      "\n",
      "Full Confusion Matrix:\n",
      "[[148  10]\n",
      " [  3  44]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming all_preds and all_labels are your predictions and true labels\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Print confusion matrix in text format\n",
    "print(\"\\nConfusion Matrix (Text Format):\")\n",
    "print(\"True Positives (Real as Real)   |\", cm[1, 1])\n",
    "print(\"False Positives (Fake as Real)  |\", cm[0, 1])\n",
    "print(\"False Negatives (Real as Fake)  |\", cm[1, 0])\n",
    "print(\"True Negatives (Fake as Fake)   |\", cm[0, 0])\n",
    "\n",
    "# Optionally: You can also print the whole matrix like this\n",
    "print(\"\\nFull Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2940b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model architecture (same as during training)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Use pre-trained ResNet18\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Change the final fully connected layer to match binary classification (real or fake)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 2)  # 2 classes (real/fake)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "264782f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('cnn_model.pth')\n",
    "print(state_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7c49963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: -4.430375099182129\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model architecture (ResNet18 in this case)\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes (e.g., 1 class)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "# Load the trained model weights (state_dict)\n",
    "model.load_state_dict(torch.load('cnn_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the image\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Make a prediction\n",
    "image = process_image('img.jpeg')  # Replace with your image path\n",
    "# image = process_image('original.png')  # Replace with your image path\n",
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    output = model(image)\n",
    "\n",
    "# If it's a classification problem (1 class), we can directly output the prediction\n",
    "print(f'Predicted output: {output.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e9cce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: 2.531982898712158, Probability: 0.9263536930084229\n",
      "Predicted label: Real (Probability: 0.9263536930084229)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to predict \"Real\" or \"Fake\" based on raw model output\n",
    "def classify_output(output):\n",
    "    # Check the sign of the raw output\n",
    "    if output.item() >= 0:\n",
    "        label = 'Real'\n",
    "        probability = torch.sigmoid(output)  # Apply sigmoid for probability\n",
    "    else:\n",
    "        label = 'Fake'\n",
    "        probability = torch.sigmoid(output)  # Apply sigmoid for probability\n",
    "\n",
    "    # Debugging: print the output and the corresponding probability\n",
    "    print(f\"Raw output: {output.item()}, Probability: {probability.item()}\")\n",
    "    \n",
    "    return label, probability.item()\n",
    "\n",
    "# Example usage (replace with your model's raw output)\n",
    "raw_output = torch.tensor([2.531982898712158])  # Replace with your model's raw output\n",
    "\n",
    "# Call the function to get the prediction\n",
    "label, probability = classify_output(raw_output)\n",
    "\n",
    "# Print the result\n",
    "print(f'Predicted label: {label} (Probability: {probability})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d04b7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    from torchvision import models, transforms\n",
    "\n",
    "    # Function to select image and return its path\n",
    "    def select_image():\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()  # Hide the root window\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select an image\",\n",
    "            filetypes=[(\"Image Files\", \"*.png *.jpg *.jpeg *.bmp *.gif\")]\n",
    "        )\n",
    "        return file_path\n",
    "\n",
    "    # Function to preprocess image from its path\n",
    "    def transform_image(image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Function to classify the output\n",
    "    def classify_output(output):\n",
    "        prob = torch.sigmoid(output)\n",
    "        print(output.item())\n",
    "        label = 'Real' if output.item() >= 0 else 'Fake'\n",
    "        return label, prob.item()\n",
    "\n",
    "    # Load model\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 1)\n",
    "    model.load_state_dict(torch.load('cnn_model.pth', map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "\n",
    "    # Select image and process it\n",
    "    image_path = select_image()\n",
    "    if not image_path:\n",
    "        print(\"No image selected.\")\n",
    "        return\n",
    "\n",
    "    input_tensor = transform_image(image_path)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    # Interpret output\n",
    "    label, probability = classify_output(output)\n",
    "\n",
    "    # Print result\n",
    "    print(f'Predicted label: {label} (Probability: {probability})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d76aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image selected: C:/Users/shiva/Pictures/DATASET/Train/Fake/0df0d345-583b630d92fe77ec452dfa74_pan_page-1_jpg.rf.0502d1976d83c6dd582e80b9896db953.jpg\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Your custom function that does something with the image\n",
    "    print(\"Image selected:\", image_path)\n",
    "    image = Image.open(image_path)\n",
    "    image.show()  # Just shows the image for demonstration\n",
    "\n",
    "def select_image():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select an image\",\n",
    "        filetypes=[(\"Image Files\", \"*.png *.jpg *.jpeg *.bmp *.gif\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        process_image(file_path)\n",
    "\n",
    "# Call this function to trigger the file dialog\n",
    "select_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b1d3084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.3929286003112793\n",
      "Predicted label: Fake (Probability: 0.08371351659297943)\n"
     ]
    }
   ],
   "source": [
    "prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
